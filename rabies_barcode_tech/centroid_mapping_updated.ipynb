{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Centroid mapping for cell type identification",
   "id": "ab023185d08a2d57"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Updated version of `##_centroid_mapping.ipynb`",
   "id": "a5a4ffcd433f8a69"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First two cells: dependencies and helper functions",
   "id": "68fea111b87e0d2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T22:12:30.899336Z",
     "start_time": "2025-10-06T22:12:30.831169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from rich.table import Table\n",
    "from rich.console import Console\n",
    "from rich.progress import track"
   ],
   "id": "ad43d4ab5e4bfd0c",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T22:12:38.562736Z",
     "start_time": "2025-10-06T22:12:38.550806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def corr2(\n",
    "        a: np.ndarray,\n",
    "        b: np.ndarray\n",
    "):\n",
    "    \"\"\"Build corr2 (Pearson correlation coefficient) matlab function\"\"\"\n",
    "    a = np.ravel(a).astype(float) - np.mean(a)\n",
    "    b = np.ravel(b).astype(float) - np.mean(b)\n",
    "    r = np.dot(a, b) / np.sqrt(np.dot(a, a) * np.dot(b, b))\n",
    "    return r\n",
    "\n",
    "\n",
    "def corr_2_matrix(\n",
    "        a: np.ndarray,\n",
    "        b: np.ndarray\n",
    "):\n",
    "    \"\"\"corr2 func extended to 2d input matrices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    a : np.ndarray\n",
    "        2d array to correlate. Has shape (samples_a, observations).\n",
    "    b : np.ndarray\n",
    "        Another 2d array to correlate. Has shape (samples_b, observations).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    corr_matrix\n",
    "        2d array with shape (samples_a, samples_b). The value at [i, j] is the\n",
    "        Pearson correlation between the i_th sample in array `a` and j_th\n",
    "        sample in array `b`.\n",
    "\n",
    "    \"\"\"\n",
    "    # Center columns (genes)\n",
    "    mat_a = a - a.mean(axis=1, keepdims=True)  # cell x gene matrix\n",
    "    mat_b = b - b.mean(axis=1, keepdims=True)  # cell x gene matrix\n",
    "\n",
    "    # Compute numerator: X^T @ Y (dot product of centered columns)\n",
    "    numerator = mat_a @ mat_b.T  # shape: (n_cells_a, n_cells_b)\n",
    "\n",
    "    # Compute denominator: column-wise norms, shape = (n_cells1, n_cells2)\n",
    "    denominator = np.outer(\n",
    "        np.linalg.norm(mat_a, axis=1), np.linalg.norm(mat_b, axis=1))\n",
    "\n",
    "    # Pearson correlation matrix\n",
    "    corr_matrix = numerator / denominator\n",
    "    return corr_matrix\n",
    "\n",
    "\n",
    "def format_str_index(\n",
    "        index,\n",
    "        replace: str = '_'\n",
    "):\n",
    "    \"\"\"Format string index values to uppercase alphanumeric characters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    index : pd.Index | pd.Series\n",
    "        Index of labels to format as strings.\n",
    "    replace : str, optional\n",
    "        Character used to replace non-alphanumeric characters in `index`.\n",
    "        Defaults to '_'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    formatted_index : pd.Series\n",
    "        `index` with non-alphanumeric characters replaced with `replace` and\n",
    "        other characters converted to uppercase.\n",
    "    \"\"\"\n",
    "    formatted_index = index.to_series().str.replace(\n",
    "        r'[^0-9A-Za-z]', replace, regex=True).str.upper()\n",
    "    return formatted_index\n",
    "\n",
    "\n",
    "def normalize_counts(\n",
    "        count_matrix: pd.DataFrame,\n",
    "        counts: pd.Series = None,\n",
    "        scalar: float = None,\n",
    "        norm_seq_depth: bool = True,\n",
    "        gene_x_cell: bool = True,\n",
    "        log: bool = True\n",
    "):\n",
    "    \"\"\"Normalize a count matrix to total UMI counts per cell.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    count_matrix : pd.DataFrame\n",
    "        One axis corresponds to identified genes. Other axis corresponds to\n",
    "        sequenced cells.\n",
    "    counts : pd.Series, optional\n",
    "        Total UMI counts per cell_id (index). Useful if `count_matrix` is a\n",
    "        filtered version of a larger DataFrame.\n",
    "        Defaults to None, in which case counts are summed from `count_matrix`.\n",
    "    scalar : float, optional\n",
    "        Scale factor with which to adjust normalized counts.\n",
    "        Defaults to None, in which case counts are not scaled.\n",
    "    norm_seq_depth : bool, optional\n",
    "        If True, normalize counts to total UMIs per cell and scale output\n",
    "        values by `scalar`.\n",
    "        Defaults to False.\n",
    "    gene_x_cell : bool, optional\n",
    "        If True, `count_matrix` is a [gene, cell] matrix. If False,\n",
    "        `count_matrix` is a [cell, gene] matrix.\n",
    "    log : bool, optional\n",
    "        If True, take logarithm of counts + 1\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    new_counts : pd.DataFrame\n",
    "        `count_matrix` normalized to total UMI counts per cell and scaled.\n",
    "    \"\"\"\n",
    "    new_counts = counts\n",
    "    if norm_seq_depth:\n",
    "        counts = counts if counts is not None else count_matrix.sum(\n",
    "            numeric_only=True, axis=int(not gene_x_cell))\n",
    "        new_counts = count_matrix.divide(counts, axis=int(gene_x_cell))\n",
    "        new_counts = new_counts if scalar is None else new_counts * scalar\n",
    "\n",
    "    new_counts = np.log1p(new_counts) if log else new_counts\n",
    "    return new_counts\n",
    "\n",
    "\n",
    "def centroid_mapping(\n",
    "        ref_counts,\n",
    "        new_counts,\n",
    "        ref_metadata,\n",
    "        write_corr_scores_df: Path,\n",
    "        type_col: str,\n",
    "        step: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    ref_counts : pd.DataFrame\n",
    "        Sparse reference matrix. Normalized and filtered to relevant genes.\n",
    "        Cell x gene matrix.\n",
    "    new_counts : pd.DataFrame\n",
    "        Sparse sample matrix. Normalized and filtered to relevant genes.\n",
    "        Cell x gene matrix.\n",
    "    ref_metadata : pd.DataFrame\n",
    "        Metadata associated with cells in `ref_counts`.\n",
    "    write_corr_scores_df : Path\n",
    "        where to save correlation coefficient matrix on local machine\n",
    "    type_col : str\n",
    "        name of column in `ref_metadata` that contains cell type labels.\n",
    "    step : int, optional\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    corr_Scores : pd.DataFrame\n",
    "    type_assignment : pd.DataFrame\n",
    "    \"\"\"\n",
    "    # add cell type column\n",
    "    ref_counts[type_col] = ref_metadata.loc[ref_counts.index, type_col]\n",
    "\n",
    "    # build centroids table of mean for each variable gene within a cluster\n",
    "    ref_counts = ref_counts.groupby(type_col).mean()  # cluster x gene\n",
    "\n",
    "    # For each cell, calculate correlations across all genes for each centroid\n",
    "    corr_scores = []\n",
    "    for i in range(0, new_counts.shape[0], step):\n",
    "        subset = new_counts.iloc[i:i+step]\n",
    "        corr_scores += [pd.DataFrame(\n",
    "            corr_2_matrix(subset.to_numpy(), ref_counts.to_numpy()),\n",
    "            index=subset.index, columns=ref_counts.index)]\n",
    "\n",
    "    corr_scores = pd.concat(corr_scores)\n",
    "    corr_scores.to_csv(write_corr_scores_df)\n",
    "\n",
    "    # Assign clusters based on highest value concordance (excluding any NaNs)\n",
    "    type_assignment = corr_scores.idxmax(axis='columns').to_frame(\n",
    "        name=type_col)\n",
    "    return corr_scores, type_assignment\n",
    "\n",
    "\n",
    "def calculate_embeddings(\n",
    "        ref_counts: pd.DataFrame,\n",
    "        new_counts: pd.DataFrame,\n",
    "        ref_umap: pd.DataFrame,\n",
    "        umap_cols: list,\n",
    "        type_col: str,\n",
    "        use_median: bool,\n",
    "        knn: int,\n",
    "        step: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    ref_counts :\n",
    "        sparse reference matrix\n",
    "    new_counts :\n",
    "        sparse sample matrix\n",
    "    ref_umap :\n",
    "        coordinates representing the embeddings for each cell from the\n",
    "        reference atlas\n",
    "    umap_cols :\n",
    "        umap coord column names in `ref_umap`\n",
    "    type_col : str\n",
    "        name of column in `ref_metadata` that contains cell type labels.\n",
    "    use_median : bool\n",
    "        whether to calculate the median or a weighted average for the query\n",
    "        embeddings\n",
    "    knn : int\n",
    "        number of neighbors to consider when selecting median/weighted mean\n",
    "    step : int\n",
    "        Number of cells to map per iteration\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    assignment_positions : pd.DataFrame\n",
    "    \"\"\"\n",
    "    # chunk over cells in new dataset\n",
    "    assignment_positions = {}\n",
    "    for i in range(0, new_counts.shape[0], step):\n",
    "        sub_new = new_counts.iloc[i:i+step]\n",
    "        corr_scores = []\n",
    "\n",
    "        # chunk over cells in reference dataset\n",
    "        for j in range(0, ref_counts.shape[0], step):\n",
    "            sub_ref = ref_counts.iloc[j:j+step]\n",
    "\n",
    "            # compute new_cells x ref_cells expression correlation matrix\n",
    "            corr_scores += [pd.DataFrame(\n",
    "                corr_2_matrix(sub_new.to_numpy(), sub_ref.to_numpy()),\n",
    "                index=sub_new.index, columns=sub_ref.index)]\n",
    "\n",
    "        # join batches by new cell id and iterate over new_cells\n",
    "        for new_cell_id, row in pd.concat(corr_scores, axis=1).iterrows():\n",
    "            # select knn reference cells based on largest correlations\n",
    "            row = row.nlargest(knn)\n",
    "\n",
    "            # extract umap coordinates for selected reference cells\n",
    "            coords = ref_umap.loc[row.index, umap_cols].to_numpy()\n",
    "\n",
    "            # count number of mapped types for reference cells\n",
    "            n_types = len(ref_umap.loc[row.index, type_col].unique())\n",
    "\n",
    "            # take median/corr weighted average of reference UMAP coordinates\n",
    "            coords = np.median(coords, axis=0) if use_median else np.average(\n",
    "                coords, axis=0, weights=row.to_numpy())\n",
    "            assignment_positions[new_cell_id] = [n_types] + coords.tolist()\n",
    "\n",
    "    assignment_positions = pd.DataFrame.from_dict(\n",
    "        assignment_positions, orient=\"index\",\n",
    "        columns=[\"n_types\"] + list(umap_cols))\n",
    "    return assignment_positions"
   ],
   "id": "95dffcb141dd89a6",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Third cell: pipeline\n",
    "  - load reference list of variable genes\n",
    "  - chunk load, normalize, and filter reference dataset\n",
    "  - chunk load, normalize, and filter query dataset\n",
    "  - map query cells to centroids of cell types in reference dataset\n",
    "  - calculate embeddings of query cells based on reference umap coordinates"
   ],
   "id": "9857e06047c72da5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T22:12:46.829122Z",
     "start_time": "2025-10-06T22:12:46.809644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pipeline(\n",
    "        ref_counts_path: Path,\n",
    "        ref_genes_path: Path,\n",
    "        ref_metadata_path: Path,\n",
    "        query_dir: Path,\n",
    "        out_dir: Path,\n",
    "        umap_cols: list,\n",
    "        scale_factor: float = 10_000,\n",
    "        norm_seq_depth: bool = True,\n",
    "        use_median: bool = True,\n",
    "        knn: int = 10,\n",
    "        type_col: str = 'type_updated',\n",
    "        step: int = 1_000\n",
    "):\n",
    "    \"\"\"Centroid mapping pipeline\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ref_counts_path : Path\n",
    "        Path to reference count matrix.csv\n",
    "    ref_genes_path : Path\n",
    "        Path to reference list of variable genes.csv\n",
    "    ref_metadata_path : Path\n",
    "        Path to reference metadata.csv\n",
    "    query_dir : Path\n",
    "        Path to directory of query datasets.csv.\n",
    "    out_dir : Path\n",
    "        Path to save output files and plots.\n",
    "    umap_cols : list[str]\n",
    "        Names of columns in `ref_metadata_path` that contain umap coordinates.\n",
    "    scale_factor : float\n",
    "        Scale factor with which to adjust normalized counts.\n",
    "    norm_seq_depth : bool\n",
    "        Whether to normalize counts by depth and `scale_factor`.\n",
    "    use_median : bool\n",
    "        If True, compute query cell embeddings with median value. If False, use\n",
    "        weighted average.\n",
    "    knn : int\n",
    "        Number of nearest neighbors to consider when calculating query umap\n",
    "        embeddings.\n",
    "    type_col : str\n",
    "        Column name in `ref_metadata_path` that contains cell type labels.\n",
    "    step : int\n",
    "        Number of cells to load or map per iteration.\n",
    "    \"\"\"\n",
    "    corr_dir = out_dir.joinpath(\"corr_scores\")\n",
    "    plot_dir = out_dir.joinpath(\"umap_plots\")\n",
    "    for subdir in (corr_dir, plot_dir):\n",
    "        subdir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    filtered_path = out_dir.joinpath(\"filtered_normed_wang_ref.csv\")\n",
    "    total_gene_path = out_dir.joinpath(\"filtered_common_genes.csv\")\n",
    "    assignment_path = out_dir.joinpath(\"mapped_centroids.csv\")\n",
    "\n",
    "    # extract relevant genes and metadata from reference files\n",
    "    ref_variable_genes = pd.read_csv(ref_genes_path, usecols=['x'])['x']\n",
    "    ref_metadata = pd.read_csv(ref_metadata_path, index_col=0)\n",
    "\n",
    "    # clean up cell ids to match those in raw data file\n",
    "    ref_metadata.index = format_str_index(ref_metadata.index)\n",
    "\n",
    "    # only load relevant genes from reference dataset\n",
    "    print(f'\\nReference dataset file available? {filtered_path.is_file()}')\n",
    "    if not filtered_path.is_file():\n",
    "        ref_data = []\n",
    "        counts = None\n",
    "        start_time = time.time()\n",
    "        for chunk in track(  # gene x cell matrix\n",
    "                pd.read_csv(ref_counts_path, index_col=0, chunksize=step),\n",
    "                description='    loading...', total=None):\n",
    "            # clean cell ids, revert ref_data transformed with log1p\n",
    "            chunk = np.expm1(chunk)\n",
    "            chunk.columns = format_str_index(chunk.columns)\n",
    "\n",
    "            # keep running total of UMI counts per cell, for all genes\n",
    "            chunk_sum = chunk.sum(axis=0, numeric_only=True)\n",
    "            counts = chunk_sum if counts is None else counts + chunk_sum\n",
    "\n",
    "            # filter to only variable genes, cells with metadata\n",
    "            chunk = chunk.loc[\n",
    "                chunk.index.isin(ref_variable_genes),\n",
    "                chunk.columns.intersection(ref_metadata.index)]\n",
    "            ref_data += [chunk]\n",
    "\n",
    "        # concat chunked raw data, normalize, save\n",
    "        print(f'    load time: {time.time() - start_time:.2f}s')\n",
    "        start_time = time.time()\n",
    "        normalize_counts(\n",
    "            pd.concat(ref_data), counts=counts, scalar=scale_factor,\n",
    "            norm_seq_depth=norm_seq_depth, gene_x_cell=True, log=True).to_csv(\n",
    "            filtered_path)\n",
    "        print(f'    norm time: {time.time() - start_time:.2f}s')\n",
    "\n",
    "    # load and convert to cell x gene matrix\n",
    "    ref_data = pd.read_csv(filtered_path, index_col=0).T\n",
    "    common_genes = ref_data.columns\n",
    "    print(f'\\nreference cell count: {ref_data.shape[0]}')\n",
    "    print(f'variable genes count: {ref_data.shape[1]}')\n",
    "\n",
    "    '''------ iterate over query datasets ------'''\n",
    "    assignments = []\n",
    "    logging = {}\n",
    "    for query_path in track(\n",
    "            list(query_dir.glob('[!.]*.csv')), description=\"mapping...\"):\n",
    "        '''------ normalize and filter query dataset ------'''\n",
    "        query_data = normalize_counts(\n",
    "            pd.read_csv(query_path, index_col=0),  # index is gene\n",
    "            scalar=scale_factor, norm_seq_depth=norm_seq_depth,\n",
    "            gene_x_cell=True, log=True).T  # index is cell\n",
    "\n",
    "        # Identify overlapping genes between reference and query datasets\n",
    "        gg = ref_data.columns.intersection(query_data.columns)\n",
    "        query_data = query_data.loc[:, gg]\n",
    "        common_genes = common_genes.intersection(gg)\n",
    "\n",
    "        '''------ centroid mapping ------'''\n",
    "        centroid_time = time.time()\n",
    "        corr, cell_type = centroid_mapping(\n",
    "            ref_counts=ref_data.loc[:, gg], new_counts=query_data,\n",
    "            ref_metadata=ref_metadata, type_col=type_col, step=step,\n",
    "            write_corr_scores_df=corr_dir.joinpath(query_path.name))\n",
    "        centroid_time = time.time() - centroid_time\n",
    "\n",
    "        # get peak correlation for each cell\n",
    "        corr = corr.max(axis=1).to_frame(name='high_score')\n",
    "        corr[['dataset_id', 'cbc']] = corr.index.to_series().str.split(\n",
    "            '_', n=1, expand=True)\n",
    "\n",
    "        '''------ embed in reference umap ------'''\n",
    "        embedding_time = time.time()\n",
    "        coords = calculate_embeddings(\n",
    "            ref_counts=ref_data.loc[:, gg], new_counts=query_data,\n",
    "            ref_umap=ref_metadata, umap_cols=umap_cols, type_col=type_col,\n",
    "            use_median=use_median, knn=knn, step=step)\n",
    "        embedding_time = time.time() - embedding_time\n",
    "        assignments += [pd.concat([corr, cell_type, coords], axis=1)]\n",
    "\n",
    "        # log data from current loop\n",
    "        logging[query_path.name] = (len(gg), centroid_time, embedding_time)\n",
    "\n",
    "    # save list of common genes as csv\n",
    "    print(f'common genes across all datasets: {len(common_genes)}')\n",
    "    common_genes.to_series().to_csv(total_gene_path, index=False, header=False)\n",
    "\n",
    "    # print logged metadata\n",
    "    table = Table(title='Mapping Summary')\n",
    "    table.add_column('dataset', style='bold cyan')\n",
    "    table.add_column('n genes', style='magenta')\n",
    "    table.add_column('t map (sec)', style='green')\n",
    "    table.add_column('t embed (sec)', style='green')\n",
    "    for d, (g, t_c, t_e) in logging.items():\n",
    "        table.add_row(d, str(g), f'{t_c:.2f}', f'{t_e:.2f}')\n",
    "\n",
    "    Console().print(table)\n",
    "\n",
    "    assignments = pd.concat(assignments)\n",
    "    mapping = {  # desired label: [dataset_ids]\n",
    "        'SADB-19 cell': ['s1', 's2', 's3', 's4', 's5'],\n",
    "        'CVS-N2c cell': ['c1', 'c2', 'c3', 'c4'],\n",
    "        'CVS-N2c nuc': ['n1', 'n2', 'n3', 'n4'],\n",
    "        'Uninfected cell': ['u1'], 'Uninfected nuc': ['u2']}\n",
    "    for k, v in mapping.items():\n",
    "        assignments.loc[assignments['dataset_id'].isin(v), 'cbc'] = k\n",
    "\n",
    "    assignments.to_csv(assignment_path)\n",
    "\n",
    "    '''------ plot high score distribution ------'''\n",
    "    sns.set_theme(style=\"ticks\", rc={\n",
    "        \"axes.spines.right\": False, \"axes.spines.top\": False})\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.xticks(rotation=45)\n",
    "    sns.violinplot(\n",
    "        data=assignments, x='dataset_id', y='high_score', hue='dataset_id',\n",
    "        legend=False, alpha=0.5)\n",
    "    plt.savefig(\n",
    "        plot_dir.joinpath('peak correlations.png'), dpi=300,\n",
    "        bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    '''------ plot mapped type distribution ------'''\n",
    "    print(f'average number of mapped types/dataset at knn: {knn}')\n",
    "    print(assignments[[type_col, 'n_types']].groupby(type_col).mean())\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.xticks(rotation=45)\n",
    "    sns.violinplot(\n",
    "        data=assignments, x='dataset_id', y='n_types', hue='dataset_id',\n",
    "        legend=False, alpha=0.5)\n",
    "    plt.savefig(\n",
    "        plot_dir.joinpath('mapped types.png'), dpi=300,\n",
    "        bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Drop all rows where high_score is <0.2\n",
    "    total_cells = assignments.shape[0]\n",
    "    assignments = assignments.query('high_score >= 0.2')\n",
    "    print(f'\\nNumber of cells pre-thresholding: {total_cells}')\n",
    "    print(f'Number of cells post-thresholding: {assignments.shape[0]}')\n",
    "    print(f'Percentage: {100 * assignments.shape[0] / total_cells:.2f}%')\n",
    "\n",
    "    '''------ plot cell types on umap ------'''\n",
    "    unique = ref_metadata[type_col].unique()\n",
    "    hue_map = dict(zip(unique, sns.color_palette(n_colors=len(unique))))\n",
    "    kwargs = {\n",
    "        'legend': False, 'palette': hue_map, 'hue_order': unique, 's': 5,\n",
    "        'marker': '.'}\n",
    "    for dset in track(\n",
    "            assignments['dataset_id'].unique(), description='plot...'):\n",
    "        subdata = assignments[assignments['dataset_id'] == dset]\n",
    "        n_cols = 2\n",
    "        fig, axes = plt.subplots(\n",
    "            ncols=n_cols, sharex=True, sharey=True, figsize=(3 * n_cols, 3),\n",
    "            constrained_layout=True)\n",
    "        sns.scatterplot(\n",
    "            data=ref_metadata, x=umap_cols[0], y=umap_cols[1],\n",
    "            hue=type_col, ax=axes[0], **kwargs)\n",
    "        sns.scatterplot(\n",
    "            data=subdata, x=umap_cols[0], y=umap_cols[1], hue=type_col,\n",
    "            ax=axes[1], **kwargs)\n",
    "        titles = [\"reference\", dset]\n",
    "        for ax, t in zip(axes, titles):\n",
    "            ax.set_title(t)\n",
    "\n",
    "        plt.savefig(\n",
    "            plot_dir.joinpath(f'{dset} dataset.png'), dpi=300,\n",
    "            bbox_inches='tight')\n",
    "        plt.close()"
   ],
   "id": "29fbe9b0475195a6",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Final cell: entry point",
   "id": "9c29948e6970228c"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-10-06T22:14:25.481368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# root directory\n",
    "base_dir = Path('/Users/ikogbonna/Documents/Lab/Cadwell Lab/Data/barcoded_tech_data')\n",
    "\n",
    "# directory with reference files\n",
    "ref_dir = base_dir.joinpath(\"wang_ref_atlas\")\n",
    "pipeline(\n",
    "    # path to reference count matrix\n",
    "    ref_counts_path=ref_dir.joinpath(\"wang_ref.csv\"),\n",
    "    # path to reference list of variable genes\n",
    "    ref_genes_path=ref_dir.joinpath(\"ref_var_genes.csv\"),\n",
    "    # path to reference metadata file\n",
    "    ref_metadata_path=ref_dir.joinpath(\"wang_metadata.csv\"),\n",
    "    # path to directory with query datasets to analyze\n",
    "    query_dir=base_dir.joinpath(\"sparse_matrices\"),\n",
    "    # path to directory in which to save outputs\n",
    "    out_dir=base_dir.joinpath(\"outputs\"),\n",
    "    # labels of umap coordinate columns\n",
    "    umap_cols=[\"umap_1\", \"umap_2\"],\n",
    "    # scale factor with which to normalize count matrices\n",
    "    scale_factor=10_000,\n",
    "    # whether to normalize count matrices or not\n",
    "    norm_seq_depth=True,\n",
    "    # if True, compute embeddings via median\n",
    "    use_median=True,\n",
    "    # number of neighbors to consider for embedding\n",
    "    knn=10,\n",
    "    # label of cell type column\n",
    "    type_col='type_updated',\n",
    "    # number of cells to load/analyze at a time\n",
    "    step=1_000\n",
    ")"
   ],
   "id": "548a76c53eb149b9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output()"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1642cd195cbc4da7bbf132fa25820574"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reference dataset file available? False\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
