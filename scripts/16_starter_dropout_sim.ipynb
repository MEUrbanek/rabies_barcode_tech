{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "104afece-4dac-4a6c-8d65-f5128d802f6b",
   "metadata": {},
   "source": [
    "# Starter Drop-out Simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b853dbe-5368-4f66-8437-9539a4857507",
   "metadata": {},
   "source": [
    "Jupyter notebook containing analysis for examining the impact of simulated starter drop-out on connectivity enrichment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44910992-586d-4766-bac0-3f916b730393",
   "metadata": {},
   "source": [
    "This notebook has some baseline calculations to model the impact of capture effiency on correctly IDing single starter networks. The final section simulates starter drop-out from actual data and reconstruct connectivity matrices based on newly called single starter networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0050abaf-d986-4f48-8467-1cff6fae8e57",
   "metadata": {},
   "source": [
    "Input for this notebook requires:\n",
    "1) processed_barcodes_df, derived from 15_connectivity_analysis.ipynb\n",
    "2) metadata_df.csv, derived from 15_connectivity_analysis.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a50bb7-0b7e-4bd6-9169-a5458cecbd06",
   "metadata": {},
   "source": [
    "Output includes:\n",
    "1) Plots correpsonding to modeling of no starter vs. single starter vs. dual starter network calls as a function of starter drop-out\n",
    "2) Plots related to connectivity and cell type proportions at a range of simulated cell capture effiencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db754c61-c0e5-4254-a291-c485d118c0a0",
   "metadata": {},
   "source": [
    "Module and their versions used when generating figures for the paper can be found in 'requirements.txt', which is stored in our GitHub repository: https://github.com/MEUrbanek/rabies_barcode_tech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63710b3-8e32-44a6-be22-5b16baba5f10",
   "metadata": {},
   "source": [
    "This code was last amended by Maddie Urbanek on 12/16/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c67512-e56a-420c-8b0e-d388a2712fc0",
   "metadata": {},
   "source": [
    "## Notebook set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44440608-eeb8-4e7b-a233-4e6d1899bb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27c64ef4-126f-4610-8263-ed68e03187b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activate operating system interfacing with JupyterLab\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4fa9e62-2c0d-49e2-8a7f-321e063b974d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change path name in function below to top-most directory containing data\n",
    "os.chdir('/Users/maddieurbanek/Desktop/revision_data/resubmission/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14aa4ee0-9fc2-484d-9f84-92d6abab3798",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For module versions, see requirements.txt on Github (LINK TO REFRESHED GITHUB)\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import networkx as nx\n",
    "from scipy import stats\n",
    "import scipy\n",
    "import math\n",
    "import time\n",
    "import upsetplot\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "plt.rc('font', family='Arial')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc4263b-4ac5-421f-92a5-4e4bcf6d508e",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78df4e07-8b2a-4b48-b17a-f2776f935b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import processed_barcodes_df, which should contain UMI and SBARRO filtered barcodes\n",
    "processed_barcodes_df=pd.read_table('./connectivity/processed_barcodes_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da9c69b-c7ff-4717-8ebe-039397e4c25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import mapped centroids as metadata_df\n",
    "metadata_df=pd.read_csv('./connectivity/metadata_df.csv')\n",
    "\n",
    "#Filter out low-score cells\n",
    "metadata_df=metadata_df.loc[metadata_df['high_score'] >0.2]\n",
    "\n",
    "#Amending column names\n",
    "metadata_df = metadata_df.rename(columns={'type_updated': 'celltype', 'Unnamed: 0':'cellbarcode','dataset_id':'datasetid'})\n",
    "metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31015892-4e28-46d4-b5bb-f34d1bf23f52",
   "metadata": {},
   "source": [
    "## Modeling starter drop-out metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31d0829-9ab7-4837-b7bc-a0d6b4c2667e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make line plot showing how risk of calling single-starter that's actually dual-starter decreases as a function of helper capture\n",
    "drop_out_risk=pd.DataFrame([.1,.2,.3,.4,.5,.6,.7,.8,.9,1])\n",
    "drop_out_risk.columns=['capture']\n",
    "drop_out_risk['both_observed']=drop_out_risk['capture']*drop_out_risk['capture']\n",
    "drop_out_risk['none_observed']=(1-drop_out_risk['capture'])*(1-drop_out_risk['capture'])\n",
    "drop_out_risk['one_observed']=1-drop_out_risk['none_observed']-drop_out_risk['both_observed']\n",
    "drop_out_risk['risk_of_calling_one_not_two']=drop_out_risk['one_observed']/drop_out_risk['both_observed']\n",
    "\n",
    "drop_out_risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916ee8a0-a99b-45be-8179-9af41f770d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting risk column\n",
    "plt.plot(drop_out_risk['capture'],drop_out_risk['both_observed'],color='blue',label='Capture Both')\n",
    "plt.plot(drop_out_risk['capture'],drop_out_risk['one_observed'],color='green',label='Capture One')\n",
    "plt.plot(drop_out_risk['capture'],drop_out_risk['none_observed'],color='black',label='Capture None')\n",
    "plt.legend()\n",
    "plt.xlabel('% of Starters Capture')\n",
    "plt.ylabel('False Single Starter Call/True Dual Starter Call')\n",
    "plt.savefig('/Users/maddieurbanek/Desktop/revision_data/resubmission/figs/sfig_conn/starter_dropout/risk_dynamics.pdf')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19f4a9d-2e4c-47fa-b58c-30b3fd362432",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting risk column\n",
    "plt.plot(drop_out_risk['capture'],drop_out_risk['risk_of_calling_one_not_two'])\n",
    "plt.axhline(y=1, color='black', linestyle=':')\n",
    "plt.xlabel('% of Starters Capture')\n",
    "plt.ylabel('False Single Starter Call/True Dual Starter Call')\n",
    "plt.savefig('/Users/maddieurbanek/Desktop/revision_data/resubmission/figs/sfig_conn/starter_dropout/false_single_risk.pdf')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53898ca7-a866-47e1-bbfc-b7f773e3952f",
   "metadata": {},
   "source": [
    "## Starter drop-out simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbcc210-c1ab-497f-98e9-8d2f4f3f5ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate null matrix proportions\n",
    "#Build null\n",
    "\n",
    "#Subsetting metadata down to neuronal populations\n",
    "subset_metadata_df = metadata_df[metadata_df.subclass != 'Microglia']\n",
    "subset_metadata_df = subset_metadata_df[subset_metadata_df.subclass != 'Unknown']\n",
    "subset_metadata_df = subset_metadata_df[subset_metadata_df.subclass != 'Vascular']\n",
    "subset_metadata_df = subset_metadata_df[subset_metadata_df.subclass != 'Astrocyte']\n",
    "subset_metadata_df = subset_metadata_df[subset_metadata_df.subclass != 'Oligo']\n",
    "subset_metadata_df = subset_metadata_df[subset_metadata_df.subclass != 'IN-Mix-LAMP5']\n",
    "\n",
    "uninfected_df = subset_metadata_df.loc[subset_metadata_df['datasetid'].isin(['u1','u2'])].groupby('subclass').count()['datasetid'].reset_index()\n",
    "uninfected_df['proportion of celltype in nonstarter cells'] = uninfected_df['datasetid']/uninfected_df['datasetid'].sum()\n",
    "\n",
    "#Rename 'datasetid' metadata column since it was overwritten by .count() function in preceding lines\n",
    "uninfected_df.rename(columns = {'datasetid':'number of cells'},inplace = True)\n",
    "\n",
    "#Clarify cell type assignments here should be used to generate non_starter null matrix populations\n",
    "uninfected_df.rename(columns = {'subclass':'non_starter_cell_type'},inplace = True)\n",
    "pooled_null = uninfected_df\n",
    "pooled_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096b40d4-9781-40b3-abe8-a52fb9289476",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build function for randomly downsampling\n",
    "from tqdm import tqdm\n",
    "\n",
    "from warnings import simplefilter\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "def downsample_helpers(processed_barcodes_df, #unfiltered processed_barcodes_df\n",
    "                       percent_to_take, #capture efficiency\n",
    "                       bootstraps, #number of times to pull helpers and calculate average number of connections from\n",
    "                       metadata_df,\n",
    "                       pooled_null\n",
    "                      ):\n",
    "\n",
    "    #Pull list of putative helpers to choose from\n",
    "    temp=processed_barcodes_df[processed_barcodes_df['helper'] == 'starter']\n",
    "    starters_to_pull=int(temp['CBC'].nunique()*percent_to_take)\n",
    "    print('Number of starters used to calculate connections:')\n",
    "    print(starters_to_pull)\n",
    "\n",
    "    bootstrapped_props=pd.DataFrame()\n",
    "    bootstrapped_counts=pd.DataFrame()\n",
    "    bootstrapped_enrichment_scores=pd.DataFrame()\n",
    "    \n",
    "    for n in tqdm(range(bootstraps)):\n",
    "        #Get your starter pool\n",
    "        starters=(np.random.choice(temp['CBC'].unique(),starters_to_pull,replace=False))\n",
    "        #Pull all the cells from the processed_barcode_dataframe with the same barcodes as these helpers\n",
    "        temp_1=temp[temp['CBC'].isin(starters)]\n",
    "        to_pull=temp_1['barcode'].unique()\n",
    "        downsampled_pbdf=processed_barcodes_df[processed_barcodes_df['barcode'].isin(to_pull)]\n",
    "\n",
    "        df = downsampled_pbdf.loc[(downsampled_pbdf['single_starter_barcode'] == 'y') & (downsampled_pbdf['starter/nonstarter barcode'] == 'both')]\n",
    "        cell_type_dict = dict(zip(df['CBC'],df['celltypes']))\n",
    "        starter_df = df.loc[df['helper'] == 'starter'].groupby(['datasetid','CBC'])['barcode'].apply(set).reset_index()\n",
    "        starter_dict = dict(zip(starter_df['CBC'],starter_df['barcode']))\n",
    "        nonstarter_df = df.loc[df['helper'] == 'nonstarter'].groupby(['CBC'])['barcode'].apply(set).reset_index()\n",
    "        nonstarter_dict = dict(zip(nonstarter_df['CBC'],nonstarter_df['barcode']))\n",
    "        for starter in starter_dict.keys():\n",
    "            matching_cells = []\n",
    "            starter_barcode_set = starter_dict[starter]\n",
    "            for nonstarter in nonstarter_dict.keys():\n",
    "                nonstarter_barcode_set = nonstarter_dict[nonstarter]\n",
    "                if len(starter_barcode_set.intersection(nonstarter_barcode_set))>=1:\n",
    "                    matching_cells.append(nonstarter)\n",
    "            starter_df.loc[starter_df['CBC'] == starter, 'matching_cells'] = str(matching_cells)\n",
    "        \n",
    "        #Process barcode data like normal\n",
    "        starter_df['matching_cells'] = starter_df['matching_cells'].apply(ast.literal_eval)\n",
    "        starter_df.rename(columns = {'CBC':'starter CBC', 'barcode':'starter barcodes', 'matching_cells':'non-starter CBC'}, inplace = True)\n",
    "        starter_df['starter cell_type'] = starter_df['starter CBC'].apply(lambda x: cell_type_dict[x])\n",
    "        conn_0_compiling = starter_df[['datasetid','starter cell_type','starter CBC','non-starter CBC']]\n",
    "        conn_0_compiling['number of non-starters associated'] = conn_0_compiling['non-starter CBC'].apply(lambda x: len(x))\n",
    "        conn_0_compiling['number of cells in single-starter network'] = conn_0_compiling['non-starter CBC'].apply(lambda x: len(x) +1)\n",
    "        conn_1_compiling = starter_df.explode('non-starter CBC')\n",
    "        conn_1_compiling['non-starter cell_type'] = conn_1_compiling['non-starter CBC'].apply(lambda x: cell_type_dict[x])\n",
    "        conn_1_compiling = conn_1_compiling[['datasetid','starter cell_type','starter CBC','non-starter cell_type','non-starter CBC']]\n",
    "        conn_1_compiling['conn_type'] = conn_1_compiling['non-starter cell_type'] +'+'+ conn_1_compiling['starter cell_type'] \n",
    "        conn_1_compiling['connection'] = conn_1_compiling.apply(lambda x: (x['non-starter CBC'], x['starter CBC'], {'w':1}), axis=1)\n",
    "        feature_dict = dict(zip(metadata_df['cellbarcode'],metadata_df['broad_class']))\n",
    "        conn_1_compiling['pre_broad_class'] = conn_1_compiling['non-starter CBC'].apply(lambda x: feature_dict[x])\n",
    "        conn_1_compiling['post_subclass'] = conn_1_compiling['starter CBC'].apply(lambda x: feature_dict[x])\n",
    "        feature_dict = dict(zip(metadata_df['cellbarcode'],metadata_df['subclass']))\n",
    "        conn_1_compiling['pre_subclass'] = conn_1_compiling['non-starter CBC'].apply(lambda x: feature_dict[x])\n",
    "        conn_1_compiling['post_subclass'] = conn_1_compiling['starter CBC'].apply(lambda x: feature_dict[x])\n",
    "        conn_1_compiling['subclass_conn']=conn_1_compiling['pre_subclass'] + '+' + conn_1_compiling['post_subclass']\n",
    "        subset_conn_1_compiling = conn_1_compiling[conn_1_compiling.post_subclass != 'Vascular']\n",
    "        subset_conn_1_compiling = subset_conn_1_compiling[subset_conn_1_compiling.pre_subclass != 'Vascular']\n",
    "        subset_conn_1_compiling = subset_conn_1_compiling[subset_conn_1_compiling.post_subclass != 'IN-Mix-LAMP5']\n",
    "        subset_conn_1_compiling = subset_conn_1_compiling[subset_conn_1_compiling.pre_subclass != 'IN-Mix-LAMP5']\n",
    "        subset_conn_1_compiling = subset_conn_1_compiling[subset_conn_1_compiling.post_subclass != 'Microglia']\n",
    "        subset_conn_1_compiling = subset_conn_1_compiling[subset_conn_1_compiling.pre_subclass != 'Microglia']\n",
    "        subset_conn_1_compiling = subset_conn_1_compiling[subset_conn_1_compiling.post_subclass != 'Astrocyte']\n",
    "        subset_conn_1_compiling = subset_conn_1_compiling[subset_conn_1_compiling.pre_subclass != 'Astrocyte']\n",
    "        subset_conn_1_compiling = subset_conn_1_compiling[subset_conn_1_compiling.post_subclass != 'Oligo']\n",
    "        subset_conn_1_compiling = subset_conn_1_compiling[subset_conn_1_compiling.pre_subclass != 'Oligo']\n",
    "        connection_counts=pd.DataFrame(subset_conn_1_compiling['subclass_conn'].value_counts())\n",
    "        bootstrapped_counts = pd.concat([bootstrapped_counts,connection_counts],axis=1,sort=False)\n",
    "        connection_counts.loc['total_counts'] = connection_counts.sum(numeric_only=True, axis=0)\n",
    "        connection_props = connection_counts.divide(connection_counts.loc['total_counts'], axis=1)\n",
    "        connection_props.drop(['total_counts'])\n",
    "        bootstrapped_props = pd.concat([bootstrapped_props,connection_props],axis=1,sort=False)\n",
    "\n",
    "        #Calculate average enrichment scores\n",
    "        #Scaling null matrix\n",
    "        starters = subset_conn_1_compiling.drop_duplicates(subset='starter CBC')\n",
    "        starters.rename(columns={'starter cell_type': 'starter_cell_type'},inplace=True)\n",
    "        feature_dict = dict(zip(metadata_df['cellbarcode'],metadata_df['subclass']))\n",
    "        starters['starter_cell_type'] = starters['starter CBC'].apply(lambda x: feature_dict[x])\n",
    "        starter_proportions=starters.groupby('starter_cell_type').count()['datasetid'].reset_index()\n",
    "        starter_proportions['proportion of celltype in starter cells'] = starter_proportions['datasetid']/starter_proportions['datasetid'].sum()\n",
    "        starter_proportions.rename(columns = {'datasetid':'number of cells'},inplace = True)\n",
    "        starter_proportions.rename(columns = {'celltype':'starter_cell_type'},inplace = True)\n",
    "        null_df = pooled_null.merge(starter_proportions, how='cross')\n",
    "        null_df['proportion of connections'] = null_df['proportion of celltype in nonstarter cells']*null_df['proportion of celltype in starter cells']\n",
    "        null_df['pre-post'] = null_df['non_starter_cell_type'] + \"+\" + null_df['starter_cell_type']\n",
    "        \n",
    "        #Now observed\n",
    "        feature_dict = dict(zip(metadata_df['cellbarcode'],metadata_df['subclass']))\n",
    "        subset_conn_1_compiling['starter cell_type'] = subset_conn_1_compiling['starter CBC'].apply(lambda x: feature_dict[x])\n",
    "        subset_conn_1_compiling['non-starter cell_type'] = subset_conn_1_compiling['non-starter CBC'].apply(lambda x: feature_dict[x])\n",
    "        conn_matrix_df = subset_conn_1_compiling.groupby(['non-starter cell_type','starter cell_type']).count()[['conn_type']].reset_index()\n",
    "        conn_matrix_df.rename(columns = {'conn_type':'observed connections'}, inplace = True)\n",
    "        conn_matrix_df['pre-post'] = conn_matrix_df['non-starter cell_type'] + '+' + conn_matrix_df['starter cell_type']\n",
    "        missing_connections = list((set(null_df['pre-post'])).difference(set(conn_matrix_df['pre-post'])))\n",
    "        df_new = pd.DataFrame({\n",
    "            'pre-post':missing_connections,\n",
    "            'observed connections':[0]*len(missing_connections),\n",
    "            'non-starter cell_type': [x.split('+')[0] for x in missing_connections],\n",
    "            'starter cell_type': [x.split('+')[1] for x in missing_connections],\n",
    "        })\n",
    "        conn_matrix_df = pd.concat([conn_matrix_df,df_new])\n",
    "        conn_matrix_df['proportion of connections'] = conn_matrix_df['observed connections'] / conn_matrix_df['observed connections'].sum()\n",
    "        \n",
    "        #Calculate enrichment scores\n",
    "        #Add any missing connections that aren't in the uninfected datasets \n",
    "        missing_connections = list((set(conn_matrix_df['pre-post'])).difference(set(null_df['pre-post'])))\n",
    "\n",
    "        df_new = pd.DataFrame({\n",
    "            'pre-post':missing_connections,\n",
    "            'proportion of connections':[0]*len(missing_connections),   \n",
    "        })\n",
    "\n",
    "        null_df = pd.concat([null_df,df_new])\n",
    "        null_dict = dict(zip(null_df['pre-post'],null_df['proportion of connections']))\n",
    "        conn_matrix_df['null proportion of connections'] = conn_matrix_df['pre-post'].apply(lambda x: null_dict[x])\n",
    "        conn_matrix_df['expected connections']=conn_matrix_df['observed connections'].sum() * conn_matrix_df['null proportion of connections']\n",
    "        conn_matrix_df['expected connections']=conn_matrix_df['expected connections']+1\n",
    "        conn_matrix_df['observed connections']=conn_matrix_df['observed connections']+1\n",
    "        conn_matrix_df['ratio of proportion of connections/null'] = ((conn_matrix_df['observed connections']) / (conn_matrix_df['expected connections']))\n",
    "        conn_matrix_df['log10 ratio of proportion of connections/null'] = (np.log10(conn_matrix_df['ratio of proportion of connections/null']))\n",
    "        conn_matrix_df=conn_matrix_df.set_index('pre-post')\n",
    "        enrichment_scores=conn_matrix_df[['log10 ratio of proportion of connections/null']]\n",
    "        bootstrapped_enrichment_scores = pd.concat([bootstrapped_enrichment_scores,enrichment_scores],axis=1,sort=False)\n",
    "    \n",
    "    #Calculate average proportions across bootstraps\n",
    "    bootstrapped_props=bootstrapped_props.fillna(0)\n",
    "    average = pd.DataFrame(bootstrapped_props.mean(axis=1))\n",
    "    bootstrapped_props['sem'] = bootstrapped_props.sem(axis=1)\n",
    "    bootstrapped_props['average']=average\n",
    "\n",
    "    bootstrapped_counts=bootstrapped_counts.fillna(0)\n",
    "    average = pd.DataFrame(bootstrapped_counts.mean(axis=1))\n",
    "    bootstrapped_counts['sem'] = bootstrapped_counts.sem(axis=1)\n",
    "    bootstrapped_counts['average']=average\n",
    "\n",
    "    bootstrapped_enrichment_scores=bootstrapped_enrichment_scores.fillna(0)\n",
    "    average = pd.DataFrame(bootstrapped_enrichment_scores.mean(axis=1))\n",
    "    bootstrapped_enrichment_scores['sem'] = bootstrapped_enrichment_scores.sem(axis=1)\n",
    "    bootstrapped_enrichment_scores['average']=average\n",
    "\n",
    "\n",
    "    #Export bootstrapped proportions\n",
    "    bootstrapped_counts.to_csv(f'/Users/maddieurbanek/Desktop/revision_data/resubmission/data/connectivity/starter_drop_out_sim/{percent_to_take}_{bootstraps}_counts.csv')\n",
    "    bootstrapped_props.to_csv(f'/Users/maddieurbanek/Desktop/revision_data/resubmission/data/connectivity/starter_drop_out_sim/{percent_to_take}_{bootstraps}_props.csv')\n",
    "    bootstrapped_enrichment_scores.to_csv(f'/Users/maddieurbanek/Desktop/revision_data/resubmission/data/connectivity/starter_drop_out_sim/{percent_to_take}_{bootstraps}_enrichment.csv')\n",
    "    \n",
    "    return bootstrapped_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20806ee5-8f2c-4e86-866c-2ddd55f03352",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run simulation for each capture efficiency rate, 50 iterations\n",
    "simulation=downsample_helpers(processed_barcodes_df,\n",
    "                   1,\n",
    "                   50, metadata_df, pooled_null)\n",
    "\n",
    "simulation=downsample_helpers(processed_barcodes_df,\n",
    "                   0.9,\n",
    "                   50, metadata_df, pooled_null)\n",
    "\n",
    "simulation=downsample_helpers(processed_barcodes_df,\n",
    "                   0.8,\n",
    "                   50, metadata_df, pooled_null)\n",
    "\n",
    "simulation=downsample_helpers(processed_barcodes_df,\n",
    "                   0.7,\n",
    "                   50, metadata_df, pooled_null)\n",
    "\n",
    "simulation=downsample_helpers(processed_barcodes_df,\n",
    "                   0.6,\n",
    "                   50, metadata_df, pooled_null)\n",
    "\n",
    "simulation=downsample_helpers(processed_barcodes_df,\n",
    "                   0.5,\n",
    "                   50, metadata_df, pooled_null)\n",
    "\n",
    "simulation=downsample_helpers(processed_barcodes_df,\n",
    "                   0.4,\n",
    "                   50, metadata_df, pooled_null)\n",
    "\n",
    "simulation=downsample_helpers(processed_barcodes_df,\n",
    "                   0.3,\n",
    "                   50, metadata_df, pooled_null)\n",
    "\n",
    "simulation=downsample_helpers(processed_barcodes_df,\n",
    "                   0.2,\n",
    "                   50, metadata_df, pooled_null)\n",
    "\n",
    "simulation=downsample_helpers(processed_barcodes_df,\n",
    "                   0.1,\n",
    "                   50, metadata_df, pooled_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ea710a-4efd-4282-b630-bcdc98d5ad5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in previous simulations\n",
    "temp=pd.DataFrame()\n",
    "\n",
    "sim_list=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "for i in sim_list:\n",
    "    sim=pd.read_table(f'/Users/maddieurbanek/Desktop/revision_data/resubmission/data/connectivity/starter_drop_out_sim/{i}_50_props.csv',delimiter=',')\n",
    "    sim=sim.set_index('subclass_conn')\n",
    "    sim=sim[['average','sem']]\n",
    "    sim.columns=[f'average_{i}',f'sem_{i}']\n",
    "    temp = pd.concat([temp,sim],axis=1,sort=False)\n",
    "\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2316ab4-591a-4695-bcb2-646369379390",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split pooled dataframe into starter-type specific\n",
    "temp['subclass_connections']=temp.index\n",
    "temp\n",
    "\n",
    "temp[['pre', 'starter']] = temp['subclass_connections'].str.split('+', expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca74f3c-a936-49ed-bc12-affe3e65d7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cr=temp[temp['starter'] == 'Cajal-Retzius cell']\n",
    "imm=temp[temp['starter'] == 'EN-Immature']\n",
    "l23=temp[temp['starter'] == 'EN-L2_3-IT']\n",
    "l4=temp[temp['starter'] == 'EN-L4-IT']\n",
    "dl=temp[temp['starter'] == 'EN-Deep Layer']\n",
    "mge=temp[temp['starter'] == 'IN-MGE']\n",
    "cge=temp[temp['starter'] == 'IN-CGE']\n",
    "dge=temp[temp['starter'] == 'IN-DGE']\n",
    "ipc=temp[temp['starter'] == 'IPC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78844b2-b6b3-4064-9bb2-33ee749f8e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make function for plotting proportions of connections for each cell type\n",
    "def starter_sim_formatting(dataset,celltype):\n",
    "\n",
    "    plt.rc('font', family='Arial', size=14)\n",
    "    \n",
    "    dataset=dataset.drop(columns=['subclass_connections', 'starter'])\n",
    "    dataset=dataset.set_index('pre')\n",
    "    averages=dataset[['average_0.1','average_0.2','average_0.3','average_0.4','average_0.5','average_0.6','average_0.7','average_0.8','average_0.9','average_1']]\n",
    "    averages.columns=[.1,.2,.3,.4,.5,.6,.7,.8,.9,1]\n",
    "    transposed=averages.T\n",
    "\n",
    "    sem=dataset[['sem_0.1','sem_0.2','sem_0.3','sem_0.4','sem_0.5','sem_0.6','sem_0.7','sem_0.8','sem_0.9','sem_1']]\n",
    "    sem.columns=[.1,.2,.3,.4,.5,.6,.7,.8,.9,1]\n",
    "    t_sem=sem.T\n",
    "    t_sem\n",
    "\n",
    "    upper = transposed + t_sem\n",
    "    lower = transposed - t_sem\n",
    "\n",
    "    print(upper)\n",
    "    \n",
    "    #ax = lineplot(data=transposed, x=transposed.index, y=\"Cajal-Retzius cell\", errorbar=None,color='#d6616b')\n",
    "    #ax.fill_between(transposed.index, lower['Cajal-Retzius cell'], upper[\"Cajal-Retzius cell\"], alpha=0.2,color='#d6616b')\n",
    "\n",
    "    ax = lineplot(data=transposed, x=transposed.index, y=\"EN-L2_3-IT\", errorbar=None,color='#e7ba52')\n",
    "    ax.fill_between(transposed.index, lower['EN-L2_3-IT'], upper[\"EN-L2_3-IT\"], alpha=0.2,color='#e7ba52')\n",
    "\n",
    "    ax = lineplot(data=transposed, x=transposed.index, y=\"EN-Deep Layer\", errorbar=None,color='#843c39')\n",
    "    ax.fill_between(transposed.index, lower['EN-Deep Layer'], upper[\"EN-Deep Layer\"], alpha=0.2,color='#843c39')\n",
    "\n",
    "    ax = lineplot(data=transposed, x=transposed.index, y=\"EN-L4-IT\", errorbar=None,color='#e7cb94')\n",
    "    ax.fill_between(transposed.index, lower['EN-L4-IT'], upper[\"EN-L4-IT\"], alpha=0.2,color='#e7cb94')\n",
    "\n",
    "    ax = lineplot(data=transposed, x=transposed.index, y=\"EN-Immature\", errorbar=None,color='#8c6d31')\n",
    "    ax.fill_between(transposed.index, lower['EN-Immature'], upper[\"EN-Immature\"], alpha=0.2,color='#8c6d31')\n",
    "\n",
    "    #ax = lineplot(data=transposed, x=transposed.index, y=\"IN-MGE\", errorbar=None,color='#a55194')\n",
    "    #ax.fill_between(transposed.index, lower['IN-MGE'], upper[\"IN-MGE\"], alpha=0.2,color='#a55194')\n",
    "\n",
    "    ax = lineplot(data=transposed, x=transposed.index, y=\"IN-CGE\", errorbar=None,color='#e7969c')\n",
    "    ax.fill_between(transposed.index, lower['IN-CGE'], upper[\"IN-CGE\"], alpha=0.2,color='#e7969c')\n",
    "\n",
    "    #ax = lineplot(data=transposed, x=transposed.index, y=\"RG\", errorbar=None,color='#8ca252')\n",
    "    #ax.fill_between(transposed.index, lower['RG'], upper[\"RG\"], alpha=0.2,color='#8ca252')\n",
    "\n",
    "    #ax = lineplot(data=transposed, x=transposed.index, y=\"IPC\", errorbar=None,color='#cedb9c')\n",
    "    #ax.fill_between(transposed.index, lower['IPC'], upper[\"IPC\"], alpha=0.2,color='#cedb9c')\n",
    "\n",
    "    ax = lineplot(data=transposed, x=transposed.index, y=\"IN-DGE\", errorbar=None,color='#ce6dbd')\n",
    "    ax.fill_between(transposed.index, lower['IN-DGE'], upper[\"IN-DGE\"], alpha=0.2,color='#ce6dbd')\n",
    "\n",
    "    ax.set_xlabel('Capture Efficiency')\n",
    "    ax.set_ylabel('Proportion of Connection IDed')\n",
    "\n",
    "    plt.savefig(f'/Users/maddieurbanek/Desktop/revision_data/resubmission/figs/sfig_conn/starter_dropout/{celltype}.pdf') \n",
    "    plt.show()\n",
    "    #return transposed, upper, lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1514990-1673-4c1d-aec9-4e865e5e1c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "starter_sim_formatting(cr,'cr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f57a9d-eb32-43d0-bbe6-e836d8445f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "starter_sim_formatting(imm,'imm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67b9c4a-8503-4a93-be22-34b4784361b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "starter_sim_formatting(l23,'l23')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5361e5ce-dee1-4262-b00d-e5367292b644",
   "metadata": {},
   "outputs": [],
   "source": [
    "starter_sim_formatting(l4,'l4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7771c47-76d6-40ff-9078-aa46efa760d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "starter_sim_formatting(dl,'dl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf777364-7cf3-40c2-b667-240622ad2c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "starter_sim_formatting(mge,'mge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6963aa55-76ce-41d4-b0a9-a0efc9d53b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "starter_sim_formatting(cge,'cge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e5345d-1457-4d09-8d3a-104af09d7f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "starter_sim_formatting(dge,'dge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2fa6bd-0a22-49d8-8201-2e5396380fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "starter_sim_formatting(ipc,'ipc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3c2aa0-4bf5-48cb-b65e-646ae9dcdf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of connections per capture efficiency\n",
    "#Read in previous simulations\n",
    "average_list=[]\n",
    "sem_list=[]\n",
    "\n",
    "sim_list=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "for i in sim_list:\n",
    "    sim=pd.read_table(f'/Users/maddieurbanek/Desktop/revision_data/resubmission/data/connectivity/starter_drop_out_sim/{i}_50_counts.csv',delimiter=',')\n",
    "    sim=sim.drop(columns=['sem', 'average','subclass_conn'])\n",
    "    sim.loc['total_connections'] = sim.sum(numeric_only=True, axis=0)\n",
    "    total_connections=sim.loc['total_connections']\n",
    "    average=total_connections.mean()\n",
    "    sem=total_connections.sem()\n",
    "    average_list.append(average)\n",
    "    sem_list.append(sem)\n",
    "\n",
    "average_list\n",
    "average_conn = pd.DataFrame(list(zip(sim_list, average_list)), columns=['efficiency', 'average'])\n",
    "sem_conn = pd.DataFrame(list(zip(sim_list, sem_list)), columns=['efficiency', 'sem'])\n",
    "sem_conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fc6703-ffee-4a45-80ee-0d93c8294e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot\n",
    "ax = lineplot(data=average_conn, x=\"efficiency\", y=\"average\", errorbar=None,color='black')\n",
    "ax.fill_between(average_conn['efficiency'], average_conn['average']-sem_conn['sem'], average_conn['average']+sem_conn['sem'],color='black', alpha=0.30)\n",
    "\n",
    "ax.set_xlabel('Capture Efficiency')\n",
    "ax.set_ylabel('Number of Connections IDed')\n",
    "\n",
    "plt.savefig(f'/Users/maddieurbanek/Desktop/revision_data/resubmission/figs/sfig_conn/starter_dropout/average_number_connections.pdf') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c47a32f-c3fd-41fd-aa78-3301efcb5f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make connectivity enrichment heatmaps with different capture effiencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af24f6a7-1ccc-4251-b5b6-e5772e15cca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build function for plotting connectivity enrichment matrices\n",
    "def downsampled_conn(dataset):\n",
    "\n",
    "    #get counts+SEM\n",
    "    counts=pd.read_table(f'/Users/maddieurbanek/Desktop/revision_data/resubmission/data/connectivity/starter_drop_out_sim/{dataset}_50_counts.csv',delimiter=',')\n",
    "    counts=counts.fillna(0)\n",
    "    counts=counts.round(1)\n",
    "    counts['average'] = counts['average'].map(str)\n",
    "    counts['sem'] = counts['sem'].map(str)\n",
    "    counts['average_sem']=counts['average']+'±'+counts['sem']\n",
    "    temp=counts[['subclass_conn','average_sem']]\n",
    "    temp[['non-starter cell_type','starter cell_type']] = temp['subclass_conn'].str.split('+', expand=True)\n",
    "    temp\n",
    "    pivot2 = temp.pivot(index='non-starter cell_type', columns='starter cell_type')['average_sem']\n",
    "    pivot2=pivot2.fillna(0) \n",
    "    pivot2\n",
    "\n",
    "    #get enrichment score\n",
    "    enrichment=pd.read_table(f'/Users/maddieurbanek/Desktop/revision_data/resubmission/data/connectivity/starter_drop_out_sim/{dataset}_50_enrichment.csv',delimiter=',')\n",
    "    temp=enrichment[['pre-post','average']]\n",
    "    temp[['non-starter cell_type','starter cell_type']] = temp['pre-post'].str.split('+', expand=True)\n",
    "    temp\n",
    "    pivot = temp.pivot(index='non-starter cell_type', columns='starter cell_type')['average']\n",
    "    pivot=pivot.fillna(0) \n",
    "\n",
    "    #plot\n",
    "    #Format input for heatmap plot\n",
    "    \n",
    "    pivot = pivot.reindex(['RG','IPC','EN-Immature','EN-L2_3-IT','EN-L4-IT','EN-Deep Layer','Cajal-Retzius cell','IN-CGE','IN-MGE','IN-DGE'], level=0) \\\n",
    "    .T.reindex(['RG','IPC','EN-Immature','EN-L2_3-IT','EN-L4-IT','EN-Deep Layer','Cajal-Retzius cell','IN-CGE','IN-MGE','IN-DGE']).T\n",
    "\n",
    "    pivot2 = pivot2.reindex(['RG','IPC','EN-Immature','EN-L2_3-IT','EN-L4-IT','EN-Deep Layer','Cajal-Retzius cell','IN-CGE','IN-MGE','IN-DGE'], level=0) \\\n",
    "    .T.reindex(['RG','IPC','EN-Immature','EN-L2_3-IT','EN-L4-IT','EN-Deep Layer','Cajal-Retzius cell','IN-CGE','IN-MGE','IN-DGE']).T\n",
    "\n",
    "    plt.figure(figsize=(15,8))\n",
    "\n",
    "    #Scaling is for log10 transformation\n",
    "    sns.heatmap(pivot,  annot= pivot2, fmt = '', \n",
    "                square = True, \n",
    "                vmin=-2.0, \n",
    "                vmax =2,\n",
    "                center=0,\n",
    "               cmap = 'bwr')\n",
    "    plt.ylabel('Non-starter')\n",
    "    plt.xlabel('Starter')\n",
    "    plt.rcParams.update({'font.size': 8})\n",
    "\n",
    "    #Save figure as an .svg file\n",
    "    plt.savefig(f'/Users/maddieurbanek/Desktop/revision_data/resubmission/figs/sfig_conn/starter_dropout/conn_matrices/{dataset}_matrix.svg', bbox_inches = 'tight', format = 'svg')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ada76ba-df37-4a9d-8cd5-86aac7694356",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run downsampled connectivity\n",
    "downsampled_conn(1)\n",
    "downsampled_conn(0.9)\n",
    "downsampled_conn(0.8)\n",
    "downsampled_conn(0.7)\n",
    "downsampled_conn(0.6)\n",
    "downsampled_conn(0.5)\n",
    "downsampled_conn(0.4)\n",
    "downsampled_conn(0.3)\n",
    "downsampled_conn(0.2)\n",
    "downsampled_conn(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34faadf9-4296-4dfd-9f3d-59d46b1b212a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### If needing to reload saved simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f44a7a-4dda-43b2-b7a9-3e348e9d2029",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts=pd.read_table('/Users/maddieurbanek/Desktop/revision_data/resubmission/data/connectivity/starter_drop_out_sim/0.1_50_counts.csv',delimiter=',')\n",
    "counts=counts.fillna(0)\n",
    "counts=counts.round(1)\n",
    "\n",
    "counts['average'] = counts['average'].map(str)\n",
    "counts['sem'] = counts['sem'].map(str)\n",
    "counts['average_sem']=counts['average']+'±'+counts['sem']\n",
    "\n",
    "\n",
    "temp=counts[['subclass_conn','average_sem']]\n",
    "temp[['non-starter cell_type','starter cell_type']] = temp['subclass_conn'].str.split('+', expand=True)\n",
    "temp\n",
    "\n",
    "pivot2 = temp.pivot(index='non-starter cell_type', columns='starter cell_type')['average_sem']\n",
    "pivot2=pivot2.fillna(0) \n",
    "pivot2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d038d7ff-30ac-4f8c-9db4-16da3199daa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "enrichment=pd.read_table('/Users/maddieurbanek/Desktop/revision_data/resubmission/data/connectivity/starter_drop_out_sim/0.1_50_enrichment.csv',delimiter=',')\n",
    "enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2427344f-9f38-4706-b5e7-7c66025171c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=enrichment[['pre-post','average']]\n",
    "temp[['non-starter cell_type','starter cell_type']] = temp['pre-post'].str.split('+', expand=True)\n",
    "temp\n",
    "\n",
    "pivot = temp.pivot(index='non-starter cell_type', columns='starter cell_type')['average']\n",
    "pivot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2325958e-88e5-40a8-848d-6c08caf93cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Format input for heatmap plot\n",
    "    pivot = observed.pivot(index='non-starter cell_type', columns='starter cell_type')['log10 ratio of proportion of connections/null']\n",
    "    \n",
    "    observed['real observed connections']=observed['post_hoc_significance'] + (observed['observed connections']-1).astype(str)\n",
    "    pivot2=observed.pivot(index='non-starter cell_type', columns='starter cell_type')['real observed connections']\n",
    "\n",
    "    pivot = pivot.reindex(['RG','IPC','EN-Immature','EN-L2_3-IT','EN-L4-IT','EN-Deep Layer','Cajal-Retzius cell','IN-CGE','IN-MGE','IN-DGE'], level=0) \\\n",
    "    .T.reindex(['RG','IPC','EN-Immature','EN-L2_3-IT','EN-L4-IT','EN-Deep Layer','Cajal-Retzius cell','IN-CGE','IN-MGE','IN-DGE']).T\n",
    "\n",
    "    pivot2 = pivot2.reindex(['RG','IPC','EN-Immature','EN-L2_3-IT','EN-L4-IT','EN-Deep Layer','Cajal-Retzius cell','IN-CGE','IN-MGE','IN-DGE'], level=0) \\\n",
    "    .T.reindex(['RG','IPC','EN-Immature','EN-L2_3-IT','EN-L4-IT','EN-Deep Layer','Cajal-Retzius cell','IN-CGE','IN-MGE','IN-DGE']).T\n",
    "\n",
    "    observed['expected connections']=observed['expected connections']+1\n",
    "    observed['observed connections']=observed['observed connections']+1\n",
    "    observed.to_csv(f'/Users/maddieurbanek/Desktop/revision_data/resubmission/figs/sfig_conn/starter_dropout/conn_matrices/{dataset_id}_observation_table.csv')\n",
    "    \n",
    "    plt.figure(figsize=(15,8))\n",
    "\n",
    "    #Scaling is for log10 transformation\n",
    "    sns.heatmap(pivot,  annot= pivot2, fmt = '', \n",
    "                square = True, \n",
    "                vmin=-2.0, \n",
    "                vmax =2,\n",
    "                center=0,\n",
    "               cmap = 'bwr')\n",
    "    plt.ylabel('Non-starter')\n",
    "    plt.xlabel('Starter')\n",
    "\n",
    "    #Save figure as an .svg file\n",
    "    plt.savefig(f'/Users/maddieurbanek/Desktop/revision_data/resubmission/figs/sfig_conn/starter_dropout/conn_matrices/{dataset_id}_matrix.svg', bbox_inches = 'tight', format = 'svg')\n",
    "\n",
    "    print('Observation table:')\n",
    "    print(observed[['pre-post','observed connections']])\n",
    "    print()\n",
    "    print('Total connections:')\n",
    "    print(observed['observed connections'].sum())\n",
    "    \n",
    "    return observed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
